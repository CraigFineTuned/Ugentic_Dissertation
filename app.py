"""
UGENTIC - Ubuntu-Driven Agentic Collective Intelligence
Multi-agent IT support system with hierarchical orchestration

Entry point for the system with robust configuration management and error handling
"""

import sys
import os

# Force Python to use the local source code
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

import json
import argparse
from pathlib import Path
from typing import Optional, Dict, Any

from langchain_ollama import ChatOllama

from src.ugentic.config_manager import get_config
from src.ugentic.agents.react_agents import (
    ITManagerAgentReAct,
    InfrastructureAgentReAct,
    NetworkSupportAgentReAct,
    AppSupportAgentReAct,
    ITSupportAgentReAct,
    ServiceDeskManagerAgentReAct
)
from src.ugentic.core.rag_core import RAGCore, get_ollama_embeddings, get_text_splitter
from src.ugentic.utils.investigation_logger import InvestigationLogger
from src.ugentic.core.explicit_planning import ExplicitPlanner
from src.ugentic.core.agent_memory import AgentMemory
from src.ugentic.logging_config import setup_logging
from src.ugentic.tools.support_tools import set_rag_system


class SystemInitializationError(Exception):
    """Raised when system initialization fails"""
    pass


def initialize_llm(model_name: str, timeout: int = 30) -> ChatOllama:
    """
    Initialize LLM with proper error handling
    
    Args:
        model_name: Name of the model to use
        timeout: Timeout for LLM connections in seconds
        
    Returns:
        Initialized ChatOllama instance
        
    Raises:
        SystemInitializationError: If LLM initialization fails
    """
    try:
        print(f"\n‚úì Initializing LLM: {model_name}")
        llm = ChatOllama(model=model_name, temperature=0.7, timeout=timeout)
        print(f"  Model: {model_name}")
        print(f"  Ready for inference\n")
        return llm
    except Exception as e:
        raise SystemInitializationError(
            f"Failed to initialize LLM with model '{model_name}'\n"
            f"Error: {str(e)}\n"
            f"Ensure Ollama is running: ollama serve"
        )


def initialize_embeddings(embedding_model: str) -> Any:
    """
    Initialize embeddings model with graceful fallback
    
    Args:
        embedding_model: Name of the embedding model
        
    Returns:
        Embeddings model or None if initialization fails
    """
    try:
        print(f"‚úì Initializing Embeddings: {embedding_model}")
        embeddings = get_ollama_embeddings(embedding_model)
        print(f"  Embeddings ready\n")
        return embeddings
    except Exception as e:
        print(f"‚ö† Warning: Failed to initialize embeddings")
        print(f"  Error: {str(e)}")
        print(f"  RAG and memory will be disabled\n")
        return None


def initialize_agents(
    llm: ChatOllama,
    logger: Optional[InvestigationLogger] = None,
    planner: Optional[ExplicitPlanner] = None
) -> Dict[str, Any]:
    """
    Initialize all ReAct agents with orchestration
    
    SESSION 30 FIX: Set orchestrator reference after Infrastructure creation
    Enables upfront triage at IT Manager level
    """
    print("‚úì Initializing React Agents")
    print("  Creating specialist agents...")
    
    # Initialize specialist agents first
    agents = {
        'Network Support': NetworkSupportAgentReAct(llm=llm, logger=logger, planner=planner),
        'App Support': AppSupportAgentReAct(llm=llm, logger=logger, planner=planner),
        'IT Support': ITSupportAgentReAct(llm=llm, logger=logger, planner=planner),
        'Service Desk Manager': ServiceDeskManagerAgentReAct(llm=llm, logger=logger, planner=planner)
    }
    
    # Initialize Infrastructure with orchestration (lead agent)
    print("  Creating orchestrator (Infrastructure)...")
    agents['Infrastructure'] = InfrastructureAgentReAct(
        llm=llm,
        orchestrator=True,
        agents=agents,
        logger=logger,
        planner=planner
    )
    
    # Initialize IT Manager WITHOUT orchestrator reference
    print("  Creating IT Manager...")
    agents['IT Manager'] = ITManagerAgentReAct(llm=llm, agents=agents)
    
    # SESSION 30 FIX: Set orchestrator reference AFTER Infrastructure is created
    # This enables upfront triage at delegation layer
    print("  Linking IT Manager to Orchestrator (SESSION 30 fix)...")
    agents['IT Manager'].set_orchestrator(agents['Infrastructure'])
    
    print(f"‚úì {len(agents)} agents initialized")
    print(f"  Ubuntu Orchestration: Enabled")
    print(f"  Upfront Triage: Enabled\n")
    
    return agents


def initialize_rag_system(config, embeddings: Optional[Any]) -> Optional[RAGCore]:
    """
    Initialize RAG system with graceful fallback
    
    Args:
        config: Configuration manager
        embeddings: Embeddings model (may be None)
        
    Returns:
        Initialized RAGCore or None if unavailable
    """
    if embeddings is None:
        print("‚Ñπ RAG system skipped (embeddings unavailable)\n")
        return None
    
    try:
        print("‚úì Initializing RAG Knowledge Base")
        splitter = get_text_splitter()
        rag_system = RAGCore(embeddings, splitter, None)
        
        kb_path = Path(config.knowledge_base_dir)
        docs = list(kb_path.glob("*")) if kb_path.exists() else []
        
        if docs:
            rag_system.load_documents_from_directory(str(kb_path))
            print(f"  Loaded {len(docs)} documents")
        else:
            print(f"  No documents found (will use empty knowledge base)")
        
        print(f"  Path: {config.knowledge_base_dir}\n")
        return rag_system
    except Exception as e:
        print(f"‚ö† Warning: RAG system initialization failed")
        print(f"  Error: {str(e)}\n")
        return None


def initialize_memory_system(embeddings: Optional[Any]) -> Optional[AgentMemory]:
    """
    Initialize memory system with graceful fallback
    
    Args:
        embeddings: Embeddings model (may be None)
        
    Returns:
        Initialized AgentMemory or None if unavailable
    """
    if embeddings is None:
        print("‚Ñπ Agent Memory skipped (embeddings unavailable)\n")
        return None
    
    try:
        print("‚úì Initializing Agent Memory System")
        memory = AgentMemory(embeddings_model=embeddings)
        
        if memory.start():
            print(f"  Cross-session learning: Enabled\n")
            return memory
        else:
            print(f"‚ö† Memory system failed to start")
            print(f"  Running with logs only\n")
            return None
    except Exception as e:
        print(f"‚ö† Warning: Agent Memory not available")
        print(f"  Error: {str(e)}")
        print(f"  Running with logs only\n")
        return None


def process_user_request(
    user_input: str,
    agents: Dict[str, Any],
    rag_system: Optional[RAGCore],
    logger: InvestigationLogger
) -> Dict[str, Any]:
    """
    Process a user request through the agent system
    
    Args:
        user_input: User's problem description
        agents: Dictionary of initialized agents
        rag_system: RAG system instance
        logger: Investigation logger
        
    Returns:
        Investigation result
    """
    print(f"\n{'='*60}")
    print(f"Processing: {user_input}")
    print(f"{'='*60}\n")
    
    # Step 1: IT Manager delegates
    print("üéØ IT Manager analyzing request...")
    it_manager = agents.get('IT Manager')
    delegation = it_manager.delegate(user_input)
    target_agent_name = delegation['agent']
    print(f"   ‚Üí Delegating to: {target_agent_name}\n")
    
    # Step 2: Agent investigates with ReAct
    target_agent = agents.get(target_agent_name)
    
    # Add RAG context if available
    rag_context = []
    if rag_system:
        try:
            rag_context = rag_system.retrieve(user_input, top_k=3)
        except Exception as e:
            print(f"‚ö† Warning: RAG retrieval failed: {str(e)}")
    
    context = {
        'user_input': user_input,
        'knowledge_base': rag_context
    }
    
    # Agent investigates (ReAct + automatic orchestration)
    result = target_agent.investigate(user_input, context)
    
    # Check if collaboration is needed
    if result.get('status') == 'NEEDS_COLLABORATION' and not getattr(target_agent, 'is_orchestrator', False):
        print(f"\n{'!'*20} ESCALATION {'!'*20}")
        print(f"Escalating to orchestrator for multi-agent collaboration...")
        print(f"{'!'*48}\n")
        
        orchestrator_agent = agents.get('Infrastructure')
        if orchestrator_agent:
            result = orchestrator_agent.investigate(user_input, context)
        else:
            print("‚ö† ERROR: Orchestrator agent not found!")
    
    # Display results
    display_results(result, rag_context)
    
    return result


def display_results(result: Dict[str, Any], rag_docs: list):
    """Display investigation results to user"""
    print(f"\n{'='*60}")
    print(f"INVESTIGATION RESULT")
    print(f"{'='*60}\n")
    
    status = result.get('status')
    
    if status == 'UBUNTU_COLLABORATION_COMPLETE':
        print("‚úì UBUNTU ORCHESTRATION EXECUTED")
        print(f"\n  Collaboration ID: {result.get('collaboration_id')}")
        print(f"  Participating Agents: {', '.join(result.get('participating_agents', []))}")
        print(f"\n  Root Cause:")
        print(f"    {result.get('root_cause')}")
        print(f"\n  Solution:")
        print(f"    {result.get('solution')}")
        print(f"\n  Ubuntu Value:")
        print(f"    {result.get('ubuntu_value')}")
    
    elif status == 'RESOLVED':
        print("‚úì ISSUE RESOLVED")
        print(f"\n  Root Cause:")
        print(f"    {result.get('root_cause')}")
        print(f"\n  Solution:")
        print(f"    {result.get('solution')}")
        print(f"\n  Iterations: {result.get('iterations', 'N/A')}")
    
    elif status == 'NEEDS_COLLABORATION':
        print("‚ö† REQUIRES COLLABORATION")
        print(f"\n  Reason: {result.get('reason')}")
        print(f"  Required Agents: {', '.join(result.get('required_agents', []))}")
    
    else:
        print(f"Status: {status}")
        print(f"Details: {result}")
    
    # Show relevant knowledge base articles
    if rag_docs:
        print(f"\nüìö Relevant Knowledge Base Articles:")
        for doc in rag_docs:
            similarity = doc.get('similarity', 0)
            text = doc.get('chunk_text', 'N/A')[:150]
            print(f"   ‚Ä¢ ({similarity:.2f}) {text}...")
    
    print(f"\n{'='*60}\n")


def show_welcome_message():
    """Display welcome message and usage information"""
    print(f"\n{'='*60}")
    print(f"UGENTIC - Ubuntu IT Support System")
    print(f"Multi-Agent Orchestration with Ubuntu Philosophy")
    print(f"{'='*60}\n")
    print("Enter IT support requests (or 'quit' to exit)")
    print("\nExample requests:")
    print("  ‚Ä¢ Users experiencing slow application performance")
    print("  ‚Ä¢ Server disk space at 95%")
    print("  ‚Ä¢ VPN connectivity issues for remote users")
    print("  ‚Ä¢ Application crashes when users log in\n")


def run_demo(fast_mode: bool = False):
    """
    Main demonstration function
    
    Args:
        fast_mode: Use faster model if True
    """
    # Load configuration
    config = get_config()
    print(f"\n{'='*60}")
    print(f"UGENTIC System Initialization")
    print(f"{'='*60}\n")
    print("Configuration Summary:")
    for key, value in config.get_config_summary().items():
        print(f"  {key}: {value}")
    print()
    
    # Setup logging
    try:
        setup_logging()
        print("‚úì Logging system initialized\n")
    except Exception as e:
        print(f"‚ö† Warning: Logging initialization failed: {str(e)}\n")
    
    # Determine model
    if fast_mode:
        model_name = "gemma:2b"
        print(f"üöÄ Fast Mode: Using {model_name}\n")
    else:
        model_name = config.reasoning_model
        print(f"üß† Standard Mode: Using {model_name}\n")
    
    # Initialize systems
    try:
        # LLM
        llm = initialize_llm(model_name)
        
        # Embeddings
        embeddings = initialize_embeddings(config.embedding_model)
        
        # Investigation Logger
        print("‚úì Initializing Investigation Logger")
        logger = InvestigationLogger(base_dir=config.logs_dir)
        print(f"  Path: {config.logs_dir}\n")
        
        # Memory System
        memory = initialize_memory_system(embeddings)
        
        # Explicit Planning
        print("‚úì Initializing Explicit Planning System")
        planner = ExplicitPlanner(plans_directory=config.plans_dir)
        print(f"  Path: {config.plans_dir}\n")
        
        # Agents
        agents = initialize_agents(llm, logger=logger, planner=planner)
        
        # RAG System
        rag_system = initialize_rag_system(config, embeddings)
        
        # Connect RAG to tools
        if rag_system:
            set_rag_system(rag_system)
            print("‚úì RAG connected to IT Support tools\n")
        
        # Ready
        print(f"{'='*60}")
        print(f"‚úì SYSTEM READY")
        print(f"{'='*60}\n")
        
    except SystemInitializationError as e:
        print(f"\n‚ùå SYSTEM INITIALIZATION FAILED")
        print(f"Error: {str(e)}")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå UNEXPECTED ERROR DURING INITIALIZATION")
        print(f"Error: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # Interactive loop
    show_welcome_message()
    
    try:
        while True:
            user_input = input("Your request: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("\nüíæ Saving session summary...")
                logger.save_session_summary()
                
                # Display memory statistics if available
                if memory and logger.memory:
                    print("\n" + "="*60)
                    print("AGENT MEMORY STATISTICS")
                    print("="*60)
                    memory_stats = logger.get_memory_stats()
                    if memory_stats:
                        print(f"Total Investigations: {memory_stats.get('total_investigations', 0)}")
                        print(f"Ubuntu Collaborations: {memory_stats.get('ubuntu_investigations', 0)}")
                        print(f"Solo Investigations: {memory_stats.get('solo_investigations', 0)}")
                        print(f"Ubuntu Success Rate: {memory_stats.get('ubuntu_success_rate', 0):.1f}%")
                        print(f"Solo Success Rate: {memory_stats.get('solo_success_rate', 0):.1f}%")
                        print(f"Ubuntu Advantage: {memory_stats.get('ubuntu_advantage', 0):+.1f}%")
                    print("="*60 + "\n")
                    
                    # Stop memory server
                    try:
                        memory.stop()
                    except Exception as e:
                        print(f"Note: Memory server stop failed: {str(e)}")
                
                print("‚úì Thank you for using UGENTIC!\n")
                break
            
            if not user_input:
                continue
            
            try:
                process_user_request(user_input, agents, rag_system, logger)
            except Exception as e:
                print(f"\n‚ùå ERROR PROCESSING REQUEST")
                print(f"Error: {str(e)}\n")
    
    except KeyboardInterrupt:
        print("\n\n‚è∏ Session interrupted by user")
        logger.save_session_summary()
        if memory:
            try:
                memory.stop()
            except:
                pass
        print("‚úì Session saved\n")
    
    except Exception as e:
        print(f"\n‚ùå FATAL ERROR")
        print(f"Error: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def main():
    """Parse arguments and run"""
    parser = argparse.ArgumentParser(
        description='UGENTIC Ubuntu Multi-Agent IT Support System'
    )
    parser.add_argument(
        '--fast',
        action='store_true',
        help='Use fast model (gemma:2b) instead of configured model'
    )
    
    args = parser.parse_args()
    run_demo(fast_mode=args.fast)


if __name__ == "__main__":
    main()
